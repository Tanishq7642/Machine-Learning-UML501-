{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tanishq7642/Machine-Learning-UML501-/blob/main/Assignment_5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bP_W113_6Zob"
      },
      "source": [
        "## Assignment 5: Regression Analysis\n"
      ],
      "id": "bP_W113_6Zob"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccD0lcAA6Zoe"
      },
      "source": [
        "### Question 1: Ridge Regression with Gradient Descent\n",
        "\n",
        "**Objective:**\n",
        "1. Generate a dataset with at least seven highly correlated columns.\n",
        "2. Implement Ridge Regression using Gradient Descent Optimization.\n",
        "3. Test different learning rates (0.0001, 0.001, 0.01, 0.1, 1, 10) with regularization parameter $10^{-5}$.\n",
        "4. Choose the best parameters for minimum cost and maximum R2 score."
      ],
      "id": "ccD0lcAA6Zoe"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjcyQylh6Zof",
        "outputId": "8fbb0c2b-c100-4186-aa6a-b81d2f2dee15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Data Shape: (500, 7)\n",
            "\n",
            "Testing Learning Rates:\n",
            "LR: 0.0001, Final Cost: 95.0818\n",
            "LR: 0.001, Final Cost: 93.9456\n",
            "LR: 0.01, Final Cost: 93.7913\n",
            "LR: 0.1, Final Cost: 93.7913\n",
            "LR: 1, Final Cost: nan\n",
            "LR: 10, Final Cost: nan\n",
            "\n",
            "Best Learning Rate: None with R2 Score: -inf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:127: RuntimeWarning: overflow encountered in reduce\n",
            "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n",
            "/tmp/ipython-input-2090255761.py:42: RuntimeWarning: overflow encountered in square\n",
            "  cost = np.mean(error**2) + lambda_param * np.sum(weights**2)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# 1. Generate Data with high correlation (effective_rank < n_features creates correlation)\n",
        "X, y = make_regression(n_samples=500, n_features=7, n_informative=7, effective_rank=2, noise=10, random_state=42)\n",
        "\n",
        "print(\"Generated Data Shape:\", X.shape)\n",
        "\n",
        "# Preprocessing\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "# Add intercept column (bias)\n",
        "X = np.c_[np.ones(X.shape[0]), X]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Gradient Descent Implementation\n",
        "def ridge_gradient_descent(X, y, learning_rate, lambda_param, iterations=1000):\n",
        "    m, n = X.shape\n",
        "    weights = np.zeros(n)\n",
        "    cost_history = []\n",
        "\n",
        "    for i in range(iterations):\n",
        "        # Prediction\n",
        "        y_pred = X.dot(weights)\n",
        "\n",
        "        # Error\n",
        "        error = y_pred - y\n",
        "\n",
        "        # Gradient\n",
        "        gradient = (2/m) * X.T.dot(error) + (2 * lambda_param * weights)\n",
        "\n",
        "        # Update weights\n",
        "        weights = weights - learning_rate * gradient\n",
        "\n",
        "        # Cost (MSE + Penalty)\n",
        "        cost = np.mean(error**2) + lambda_param * np.sum(weights**2)\n",
        "        cost_history.append(cost)\n",
        "\n",
        "    return weights, cost_history\n",
        "\n",
        "# Testing Learning Rates\n",
        "learning_rates = [0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
        "lambda_val = 1e-5\n",
        "\n",
        "best_r2 = -float('inf')\n",
        "best_lr = None\n",
        "best_weights = None\n",
        "\n",
        "print(\"\\nTesting Learning Rates:\")\n",
        "for lr in learning_rates:\n",
        "    weights, costs = ridge_gradient_descent(X_train, y_train, lr, lambda_val)\n",
        "\n",
        "    # Evaluate\n",
        "    y_pred_test = X_test.dot(weights)\n",
        "\n",
        "    print(f\"LR: {lr}, Final Cost: {costs[-1]:.4f}\")\n",
        "\n",
        "\n",
        "print(f\"\\nBest Learning Rate: {best_lr} with R2 Score: {best_r2:.4f}\")"
      ],
      "id": "tjcyQylh6Zof"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVAfASTT6xT-",
        "outputId": "119d82ec-446d-4ec7-9225-68a7c5196f36"
      },
      "id": "XVAfASTT6xT-",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sglF4VqM6Zoi"
      },
      "source": [
        "### Question 2: Hitters Dataset Analysis\n",
        "\n",
        "**Objective:** Pre-process the Hitters data, perform scaling, and fit Linear, Ridge, and LASSO models."
      ],
      "id": "sglF4VqM6Zoi"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5g-Q2jL6Zoi",
        "outputId": "5311e1a3-524d-48a0-ca37-ddc0f1fdff34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hitters Dataset (First 5 rows as numpy array):\n",
            "\n",
            "[[293 66 1 30 29 14 1 293 66 1 30 29 14 'A' 'E' 446 33 20 nan 'A']\n",
            " [315 81 7 24 38 39 14 3449 835 69 321 414 375 'N' 'W' 632 43 10 475.0\n",
            "  'N']\n",
            " [479 130 18 66 72 76 3 1624 457 63 224 266 263 'A' 'W' 880 82 14 480.0\n",
            "  'A']\n",
            " [496 141 20 65 78 37 11 5628 1575 225 828 838 354 'N' 'E' 200 11 3 500.0\n",
            "  'N']\n",
            " [321 87 10 39 42 30 2 396 101 12 48 46 33 'N' 'E' 805 40 4 91.5 'N']]\n",
            "\n",
            "--- Question 2 Results ---\n",
            "Linear Regression R2: 0.38062339666128975\n",
            "Ridge Regression R2: 0.401939851567225\n",
            "LASSO Regression R2: 0.394986120390298\n",
            "\n",
            "Best Model for Hitters:\n",
            "Ridge performed best.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.100e+04, tolerance: 3.414e+03\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LinearRegression, Ridge, Lasso, RidgeCV, LassoCV\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# ---------------------------\n",
        "# DATASET PREVIEW (NUMPY STYLE)\n",
        "# ---------------------------\n",
        "data_hitters = pd.read_csv('/content/drive/MyDrive/Hitters (1).csv')\n",
        "print(\"Hitters Dataset (First 5 rows as numpy array):\\n\")\n",
        "print(data_hitters.head().values)\n",
        "\n",
        "# (a) Pre-process the data\n",
        "# Remove rows with missing values\n",
        "data_hitters = data_hitters.dropna()\n",
        "\n",
        "# Convert categorical columns to numerical using One-Hot Encoding (drop_first to avoid dummy variable trap)\n",
        "data_hitters = pd.get_dummies(data_hitters, drop_first=True)\n",
        "\n",
        "# (b) Separate input (X) and output (y) and perform scaling\n",
        "X_h = data_hitters.drop('Salary', axis=1)\n",
        "y_h = data_hitters['Salary']\n",
        "\n",
        "# Split into training and testing sets (70% train, 30% test)\n",
        "X_train_h, X_test_h, y_train_h, y_test_h = train_test_split(X_h, y_h, test_size=0.3, random_state=42)\n",
        "\n",
        "# Standard Scaling\n",
        "scaler_h = StandardScaler()\n",
        "X_train_h_scaled = scaler_h.fit_transform(X_train_h)\n",
        "X_test_h_scaled = scaler_h.transform(X_test_h)\n",
        "\n",
        "# (c) Fit Linear, Ridge, and LASSO models\n",
        "# Linear Regression\n",
        "lin_reg_h = LinearRegression()\n",
        "lin_reg_h.fit(X_train_h_scaled, y_train_h)\n",
        "\n",
        "# Ridge Regression (alpha = 0.5748)\n",
        "ridge_reg_h = Ridge(alpha=0.5748)\n",
        "ridge_reg_h.fit(X_train_h_scaled, y_train_h)\n",
        "\n",
        "# LASSO Regression (alpha = 0.5748)\n",
        "lasso_reg_h = Lasso(alpha=0.5748)\n",
        "lasso_reg_h.fit(X_train_h_scaled, y_train_h)\n",
        "\n",
        "# (d) Evaluate performance\n",
        "y_pred_lin_h = lin_reg_h.predict(X_test_h_scaled)\n",
        "y_pred_ridge_h = ridge_reg_h.predict(X_test_h_scaled)\n",
        "y_pred_lasso_h = lasso_reg_h.predict(X_test_h_scaled)\n",
        "\n",
        "score_lin_h = r2_score(y_test_h, y_pred_lin_h)\n",
        "score_ridge_h = r2_score(y_test_h, y_pred_ridge_h)\n",
        "score_lasso_h = r2_score(y_test_h, y_pred_lasso_h)\n",
        "\n",
        "print(\"\\n--- Question 2 Results ---\")\n",
        "print(\"Linear Regression R2:\", score_lin_h)\n",
        "print(\"Ridge Regression R2:\", score_ridge_h)\n",
        "print(\"LASSO Regression R2:\", score_lasso_h)\n",
        "\n",
        "print(\"\\nBest Model for Hitters:\")\n",
        "if score_ridge_h > score_lin_h and score_ridge_h > score_lasso_h:\n",
        "    print(\"Ridge performed best.\")\n",
        "elif score_lasso_h > score_lin_h and score_lasso_h > score_ridge_h:\n",
        "    print(\"LASSO performed best.\")\n",
        "else:\n",
        "    print(\"Linear Regression performed best.\")"
      ],
      "id": "j5g-Q2jL6Zoi"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_h1WBnf6Zoj"
      },
      "source": [
        "### Question 3: Cross Validation\n",
        "\n",
        "**Objective:** Explore Ridge Cross Validation (RidgeCV) and Lasso Cross Validation (LassoCV) to find the optimal alpha for the Hitters dataset and compare it with the alpha used in Q2 (0.5748)."
      ],
      "id": "m_h1WBnf6Zoj"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEMb0gNd6Zok",
        "outputId": "8d4c5268-5e83-49d8-e006-406a4ef820b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Question 3 Results ---\n",
            "Optimal Alpha found by RidgeCV: 100.0\n",
            "Optimal Alpha found by LassoCV: 10.0\n",
            "Alpha used in Q2: 0.5748\n",
            "\n",
            "Comparison:\n",
            "RidgeCV chose a different alpha, suggesting 0.5748 might not be optimal.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 17898.018819622695, tolerance: 2749.7026229058674\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 273128.9263294479, tolerance: 2749.7026229058674\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1296439.6583800577, tolerance: 2749.7026229058674\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2356108.7525069853, tolerance: 2749.7026229058674\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2721.3316660523415, tolerance: 2550.3797612488656\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3860.808043701574, tolerance: 2550.3797612488656\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 91220.73545509204, tolerance: 2630.4916917299033\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 618501.4925661627, tolerance: 2630.4916917299033\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1081853.2897529276, tolerance: 2630.4916917299033\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 167941.38276986778, tolerance: 2686.9937413239854\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1845012.127295211, tolerance: 2686.9937413239854\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2841771.9719704557, tolerance: 2686.9937413239854\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4417.373552639037, tolerance: 3016.702315249711\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6714.100918330252, tolerance: 3016.702315249711\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1367098.7469258737, tolerance: 3016.702315249711\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3285113.150630325, tolerance: 3016.702315249711\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:681: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4204391.672274825, tolerance: 3016.702315249711\n",
            "  model = cd_fast.enet_coordinate_descent_gram(\n"
          ]
        }
      ],
      "source": [
        "# RidgeCV: Built-in Cross Validation for Ridge\n",
        "# We test a range of alphas\n",
        "alphas_to_test = [0.001, 0.01, 0.1, 0.5748, 1, 10, 100]\n",
        "\n",
        "ridge_cv = RidgeCV(alphas=alphas_to_test, scoring='r2')\n",
        "ridge_cv.fit(X_train_h_scaled, y_train_h)\n",
        "\n",
        "# LassoCV: Built-in Cross Validation for Lasso\n",
        "lasso_cv = LassoCV(alphas=alphas_to_test, cv=5, random_state=42)\n",
        "lasso_cv.fit(X_train_h_scaled, y_train_h)\n",
        "\n",
        "print(\"--- Question 3 Results ---\")\n",
        "print(\"Optimal Alpha found by RidgeCV:\", ridge_cv.alpha_)\n",
        "print(\"Optimal Alpha found by LassoCV:\", lasso_cv.alpha_)\n",
        "print(\"Alpha used in Q2: 0.5748\")\n",
        "\n",
        "print(\"\\nComparison:\")\n",
        "if ridge_cv.alpha_ == 0.5748:\n",
        "    print(\"RidgeCV chose the same alpha as Q2.\")\n",
        "else:\n",
        "    print(\"RidgeCV chose a different alpha, suggesting 0.5748 might not be optimal.\")\n"
      ],
      "id": "nEMb0gNd6Zok"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwMcvNsT6Zok"
      },
      "source": [
        "### Question 4: Boston Housing Dataset Analysis\n",
        "\n",
        "**Objective:** Perform a similar regression analysis on the Boston Housing dataset."
      ],
      "id": "lwMcvNsT6Zok"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "krAp4J3g6Zol",
        "outputId": "6b484acc-c968-45a9-cf35-1a521dbcb9ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Boston Housing Dataset (First 5 rows as numpy array):\n",
            "\n",
            "[[6.3200e-03 1.8000e+01 2.3100e+00 0.0000e+00 5.3800e-01 6.5750e+00\n",
            "  6.5200e+01 4.0900e+00 1.0000e+00 2.9600e+02 1.5300e+01 3.9690e+02\n",
            "  4.9800e+00 2.4000e+01]\n",
            " [2.7310e-02 0.0000e+00 7.0700e+00 0.0000e+00 4.6900e-01 6.4210e+00\n",
            "  7.8900e+01 4.9671e+00 2.0000e+00 2.4200e+02 1.7800e+01 3.9690e+02\n",
            "  9.1400e+00 2.1600e+01]\n",
            " [2.7290e-02 0.0000e+00 7.0700e+00 0.0000e+00 4.6900e-01 7.1850e+00\n",
            "  6.1100e+01 4.9671e+00 2.0000e+00 2.4200e+02 1.7800e+01 3.9283e+02\n",
            "  4.0300e+00 3.4700e+01]\n",
            " [3.2370e-02 0.0000e+00 2.1800e+00 0.0000e+00 4.5800e-01 6.9980e+00\n",
            "  4.5800e+01 6.0622e+00 3.0000e+00 2.2200e+02 1.8700e+01 3.9463e+02\n",
            "  2.9400e+00 3.3400e+01]\n",
            " [6.9050e-02 0.0000e+00 2.1800e+00 0.0000e+00 4.5800e-01 7.1470e+00\n",
            "  5.4200e+01 6.0622e+00 3.0000e+00 2.2200e+02 1.8700e+01 3.9690e+02\n",
            "  5.3300e+00 3.6200e+01]]\n",
            "\n",
            "--- Question 4 Results (Boston Housing) ---\n",
            "Linear Regression R2: 0.7112260057484932\n",
            "Ridge Regression R2: 0.7109850548737549\n",
            "LASSO Regression R2: 0.6568527239968585\n",
            "\n",
            "Best Model for Boston Housing:\n",
            "Linear Regression performed best.\n"
          ]
        }
      ],
      "source": [
        "# Load Boston Housing Data\n",
        "data_boston = pd.read_csv('/content/drive/MyDrive/Boston_Housing.csv')\n",
        "\n",
        "print(\"Boston Housing Dataset (First 5 rows as numpy array):\\n\")\n",
        "print(data_boston.head().values)\n",
        "\n",
        "# Pre-processing\n",
        "# Check for nulls (usually none, but good practice)\n",
        "data_boston = data_boston.dropna()\n",
        "\n",
        "# Separate Input (X) and Output (y)\n",
        "# 'MEDV' is the median value of owner-occupied homes (Target)\n",
        "X_b = data_boston.drop('MEDV', axis=1)\n",
        "y_b = data_boston['MEDV']\n",
        "\n",
        "# Split Data\n",
        "X_train_b, X_test_b, y_train_b, y_test_b = train_test_split(X_b, y_b, test_size=0.3, random_state=42)\n",
        "\n",
        "# Scale Data\n",
        "scaler_b = StandardScaler()\n",
        "X_train_b_scaled = scaler_b.fit_transform(X_train_b)\n",
        "X_test_b_scaled = scaler_b.transform(X_test_b)\n",
        "\n",
        "# Fit Models\n",
        "# Linear\n",
        "lin_reg_b = LinearRegression()\n",
        "lin_reg_b.fit(X_train_b_scaled, y_train_b)\n",
        "\n",
        "# Ridge (Using Q2 alpha for consistency)\n",
        "ridge_reg_b = Ridge(alpha=0.5748)\n",
        "ridge_reg_b.fit(X_train_b_scaled, y_train_b)\n",
        "\n",
        "# Lasso (Using Q2 alpha for consistency)\n",
        "lasso_reg_b = Lasso(alpha=0.5748)\n",
        "lasso_reg_b.fit(X_train_b_scaled, y_train_b)\n",
        "\n",
        "# Evaluate\n",
        "y_pred_lin_b = lin_reg_b.predict(X_test_b_scaled)\n",
        "y_pred_ridge_b = ridge_reg_b.predict(X_test_b_scaled)\n",
        "y_pred_lasso_b = lasso_reg_b.predict(X_test_b_scaled)\n",
        "\n",
        "score_lin_b = r2_score(y_test_b, y_pred_lin_b)\n",
        "score_ridge_b = r2_score(y_test_b, y_pred_ridge_b)\n",
        "score_lasso_b = r2_score(y_test_b, y_pred_lasso_b)\n",
        "\n",
        "print(\"\\n--- Question 4 Results (Boston Housing) ---\")\n",
        "print(\"Linear Regression R2:\", score_lin_b)\n",
        "print(\"Ridge Regression R2:\", score_ridge_b)\n",
        "print(\"LASSO Regression R2:\", score_lasso_b)\n",
        "\n",
        "# Determine Best\n",
        "print(\"\\nBest Model for Boston Housing:\")\n",
        "if score_ridge_b > score_lin_b and score_ridge_b > score_lasso_b:\n",
        "    print(\"Ridge performed best.\")\n",
        "elif score_lasso_b > score_lin_b and score_lasso_b > score_ridge_b:\n",
        "    print(\"LASSO performed best.\")\n",
        "else:\n",
        "    print(\"Linear Regression performed best.\")"
      ],
      "id": "krAp4J3g6Zol"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}